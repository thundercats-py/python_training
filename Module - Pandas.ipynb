{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transcribed from FOIA Doc ID: 6689695\n",
    "\n",
    "https://archive.org/details/comp3321"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module: Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(U) This modules covers the Pandas package in Python, for working with dataframes.\n",
    "\n",
    "**Note:** This module does not include portion markings in the source PDF after this point and did not include an overall classification. It is from the same FOIA source as the other modules included in this collection. I will call out my changes if I make any."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Resource & Examples \n",
    "\n",
    "(Note: this was modified from the Pandamonium notebook by _redacted_ on nbGallery.)\n",
    "\n",
    "This resource should help people who are new to Pandas and need to explore capabilities or learn the syntax. We'll provide a few examples for each command we introduce. It's important to mention that these are not all the commands available! \n",
    "\n",
    "If you prefer video tutorials, here's a Safari series => \n",
    "\n",
    "[Data Analysis with Python and Pandas](https://learning.oreilly.com/videos/data-analysis-with/100000006A0408)\n",
    "\n",
    "Also note that Pandas documentation is available [here](https://pandas.pydata.org/pandas-docs/stable/). \n",
    "\n",
    "First we'll import and install all necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipydeps \n",
    "modules = ['pandas', 'xlrd', 'bokeh', 'numpy', \n",
    "           'requests', 'openpyxl'] \n",
    "ipydeps.pip(modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pd` is the standard abbreviation for pandas, and `np` for numpy. It's a standard convention if you're writing a program which uses pandas or numpy to import them as `pd` or `np`. You don't have to do this, but if you don't other programmers reviewing your code might get confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# This is only included to give us a sample dataframe to work with\n",
    "from bokeh.sampledata.autompg import autompg as df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataFrame \n",
    "\n",
    "The very basics of creating your own DataFrame. You might not be creating them from scratch often but you probably will create empty DataFrames like you'll see a few times further down in the guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Empty DataFrame Object\n",
    "df1 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the very basic method, create empty DataFrame but specify 4 columns and their names \n",
    "# You can also specify datatypes, index, and many other advanced things here \n",
    "df1 = pd.DataFrame(columns=('Column1', 'Column2', 'Column3', 'Column4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create testing DataFrames (a, b, c), always useful for evaluating merge/join/concat/append operations. \n",
    "a = pd.DataFrame([[1,2,3],[3,4,5]], columns=list('ABC'))\n",
    "b = pd.DataFrame([[5,2,3],[7,4,5]], columns=list('BDE'))\n",
    "c = pd.DataFrame([[11,12,13],[17,14,15]], columns=list('XYZ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from and Writing To Files \n",
    "\n",
    "Super easy in Pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV\n",
    "\n",
    "Let's write our autompg dataframe out to csv first so we have one to work this. Note: if you leave the `index` parameter set to `True`, you'll get an extra column called \"Unnamed: 0\" in your CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"autompg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now reading it in is super easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"autompg.csv\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the file contains special encoding (if it's not English for example) you can look up the encoding you need for your language or text and include that when you read the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('autompg.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify which columns you'd like to read in (if you'd prefer a subset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('autompg.csv', usecols=['name', 'mpg']) \n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your file is not a csv, and uses alternative seperators, you can specify that when you read it in. Your file does not need to have a \".csv\" extension to be read by this function, but should be a text file that represents data.\n",
    "\n",
    "_For Example,_ if you have a \".tsv\", or tab-delimited file you can specify that to pandas when reading the file in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"autompg.tsv\", index=False, sep='\\t') \n",
    "df1 = pd.read_csv('autompg.tsv', sep='\\t') \n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking on Large CSVs\n",
    "\n",
    "Often times, when working with very large CSVs you will run into errors. There are a few methods to work around these errors without installing more memory.\n",
    "\n",
    "If you don't have enough memory to directly open an entire CSV, as when they start going above 500MB-1GB+, you can _sometimes_ alleviate the problem by chunking the in-read (opening them in smaller pieces). \n",
    "\n",
    "**Note:** your numeric index will be reset each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we'll create a large DataFrame for an example \n",
    "large_df = pd.DataFrame() \n",
    "for i in range(100): \n",
    "    # ignore_index prevents the index from being reset with each DataFrame added \n",
    "    large_df = large_df.append(df1, ignore_index=True) \n",
    "large_df.to_csv(\"large_file.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk becomes the temporary dataframe containing the data of that chunk size \n",
    "for chunk in pd.read_csv('large_file.csv', chunksize=1000): \n",
    "    print(chunk.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another chunking variation \n",
    "\n",
    "If you still need to load a very large CSV into memory for deduplication or other processing reasons, there are ways to do it. This method uses a temporary DataFrame for appending, which gets dumped into a master DataFrame after 200 chunks have been processed. Clearing the temporary DataFrame every 200 chunks reduces memory overhead and improves speed during the append process. \n",
    "\n",
    "You can improve efficiency by adjusting chunksize and the interval that it dumps data into the master DataFrame. There may be more efficient ways to do this, but this is effective. At the end of the cell, we have a DataFrame `df1` which has all the data that we couldn't read all at once. \n",
    "\n",
    "**Notes:** We use ignore_index in order to have unique index values, since append will automatically preserve index values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame()\n",
    "df2 = pd.DataFrame()\n",
    "\n",
    "for counter, chunk in enumerate(pd.read_csv('large_file.csv', chunksize=1000)):\n",
    "    # Every 200 chunks, append df2 to df1, clear memory, start an empty df2\n",
    "    if (counter % 200) == 0:\n",
    "        df2 = df2.append(chunk, ignore_index=True)\n",
    "        df1 = df1.append(df2, ignore_index=True)\n",
    "        df2 = pd.DataFrame()\n",
    "    else:\n",
    "        df2 = df2.append(chunk, ignore_index=True)\n",
    "\n",
    "# Anything leftover gets appended to master dataframe (df1)\n",
    "df1 = df1.append(df2, ignore_index=True)\n",
    "\n",
    "# remove the temporary DataFrame\n",
    "\n",
    "del df2\n",
    "\n",
    "print(f\"There are {len(df1)} rows in this DataFrame.\")\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excel \n",
    "\n",
    "Use `Excelwriter` to write a `DataFrame` or multiple `DataFrames` to an Excel workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame([\n",
    "    {'Name': 'Po', 'Occupation': 'Dragon Warrior'},\n",
    "    {'Name': 'Shifu', 'Occupation': 'Sensei'}\n",
    "])\n",
    "# this just initializes the workbook\n",
    "writer = pd.ExcelWriter(\"test_workbook.xlsx\")\n",
    "# write as many DataFrames as sheets as you want\n",
    "df.to_excel(writer, \"Sheet1\")\n",
    "df2.to_excel(writer, \"Sheet2\")\n",
    "writer.save() # ,save() finishes the operation and saves the workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reading from an Excel workbook, Pandas assumes you want just the first sheet of the workbook by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel('test_workbook.xlsx')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read a specific sheet, simply include the name of the sheet in the read command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel('test_workbook.xlsx', sheet_name='Sheet2') \n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading from JSON/API\n",
    "\n",
    "This is just a very simple example to show that it's very easy for JSON or API payloads to be converted to a DataFrame, as long as the payload has a structured format that can be interpreted. \n",
    "\n",
    "Pandas can write a DataFrame to a JSON file, and also read in from a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"json_file.json\")\n",
    "from_json = pd.read_json(\"json_file.json\")\n",
    "from_json.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be done for JSON objects instead of files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_object = df.to_json() # don't specify a file and it will create a JSON object\n",
    "from_json = pd.read_json(json_object)\n",
    "from_json.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Information Summaries \n",
    "\n",
    "Now that your data is imported, we can get down to business. \n",
    "\n",
    "To retrieve basic information about your DataFrame, like the shape (column and row numbers), index values (row identifiers), DataFrame info (attributes of the object), and the count (number of values in the columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe DataFrame \n",
    "\n",
    "Summary Statistics - `DataFrame.describe()` will try to process numeric columns by running: (count, mean, standard deviation (std), min, 25%, 50%, 75%, max) output will be that summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Head and Foot of DataFrame \n",
    "\n",
    "**Note:** You can use this on most operations (especially in this guide) to get a small preview of the output instead of the entire DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 5 rows of DataFrame \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the number of rows to preview \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Last 5 rows of DataFrame \n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or Specify \n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking DataTypes \n",
    "\n",
    "It's important to know how your DataFrame will treat the data contained in specific columns, and how it will read in the columns. Pandas will attempt to automatically parse numbers as `int` or `float`, and can be asked to parse dates as datetime objects. Understanding where it succeeded and where an explicit parse statement will be needed is important, the dataframe can provide this information. \n",
    "\n",
    "**Note:** Pandas automatically uses numpy objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View column names and their associated datatype \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns where the datatype is float64 using numpy (a decimal number) \n",
    "df.select_dtypes([np.float64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns where the datatype is a numpy object (like a string) \n",
    "df.select_dtypes([np.object])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of a column \n",
    "df2 = df.copy()\n",
    "df2['mpg'] = df2['mpg'].astype(str)\n",
    "df2['mpg'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying DataFrames \n",
    "\n",
    "Modifications only work on assignment or when using `inplace=True`, which instructs the DataFrame to make the change without reassignment.\n",
    "\n",
    "See examples below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change by assignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop('cyl',axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change in place "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop('hp',axis=1, inplace=True) # inplace \n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View and Rename Columns \n",
    "\n",
    "Check all column names or Rename specific columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Column Names \n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store column names as a list \n",
    "x = list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch renaming columns requires a dictionary of the old values mapped to the new ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.rename(columns={'mpg': 'miles_per_gallon', \n",
    "                         'cyl': 'cylinders'}) \n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create New Columns \n",
    "\n",
    "Similar to a dictionary, if a column doesn't exist, this will automatically create it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will populate entire column with value specified \n",
    "df2 = df.copy()\n",
    "df2['year'] = '2017' \n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Index and Columns \n",
    "\n",
    "Access a specific column by name or row by index\n",
    "\n",
    "Change the column placeholders below to actually see working \n",
    "\n",
    "Columns Available in Practice DataFrame: (mpg, cyl, displ, hp, weight, accel, yr, origin, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Column \n",
    "df['name'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively and equivalent to above, this won't work if there are spaces in the column name \n",
    "df.name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Numeric Index, below is specifying 2nd and 3rd rows of vaLues \n",
    "df.iloc[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8y Index + Column \n",
    "df.loc[[1], ['name']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicates \n",
    "\n",
    "Important operation for reducing a DataFrame! \n",
    "\n",
    "Change the column placeholders below to actually see working \n",
    "\n",
    "Columns Available in Practice DataFrame: (mpg, cyl, displ, hp, weight, accel, yr, origin, name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first let's create some duplicates \n",
    "df2 = df.append(df, ignore_index=True) \n",
    "print(\"There are {} rows in the DataFrame.\".format(len(df2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows which contain dupiicates of another row \n",
    "df2.drop_duplicates(inplace=True) \n",
    "print (\"There are now {} rows in the DataFrame.\".format (len(df2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or specify columns to reduce the number of cells in a row that must match to be dropped \n",
    "df2 = df2.drop_duplicates(subset=['mpg']) \n",
    "\n",
    "print(\"There are now {} rows in the DataFrame.\".format(len(df2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering on Columns \n",
    "\n",
    "Filter a DataFrame based on specific column & value parameters. In the example below, we are creating a new DataFrame (df2) from our filter specifications against the sample DataFrame (df). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created new dataframe where 'cyl' vaLue == 6 \n",
    "df2 = df.loc[df['cyl'] == 6] \n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use reset_index to re-number the index values \n",
    "df2 = df.loc[df['cyl'] == 6].reset_index(drop=True) \n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['name'] == 'ford taurus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not that we don't need .loc for these operations \n",
    "df2 = df[df['mpg'] >= 16].reset_index(drop=True) \n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill or Drop the NaN or null Values \n",
    "\n",
    "Repair Empty Values or 'NaN' across DataFrame or Columns \n",
    "\n",
    "Change the column placeholders below to actually see working \n",
    "\n",
    "Columns Available in Practice DataFrame: (mpg, cyl, displ, hp, weight, accel, yr, origin, name) \n",
    "\n",
    "**Note:** `df.dropna` & `df.fillna` are modifications and will modify the sample DataFrame. Remove \"inplace=True\" from entries to prevent modification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(df.reindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we'll add some empty values \n",
    "df3 = pd.DataFrame([{'name': 'Ford Taurus'}, {'mpg': 18.0}]) \n",
    "df2 = df.append(df3, ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaN values \n",
    "df2.loc[df2['mpg'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[df2['name'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True/False Output on if columns contain null values \n",
    "df2.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of all missing values by column \n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of all missing values across all columns \n",
    "df2.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate all missing values \n",
    "df2.loc[df2.isnull().transpose().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values \n",
    "df2.fillna(0).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.fillna(0).tail().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN values \n",
    "df2.dropna().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively target a column \n",
    "df2['cyl'].fillna(0).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop row only if all columns are NaN \n",
    "df2.dropna(how='all').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop if a specific number of columns are NaN \n",
    "df2.dropna(thresh=2).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop if specific columns are NaN \n",
    "df2.dropna(subset=['displ', 'hp']).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Unique values in column \n",
    "df['mpg'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of Unique Values in column \n",
    "df['cyl'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of all entries in column \n",
    "df['hp'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of all column values \n",
    "df['hp'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of all column values \n",
    "df['cyl'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median of all column values \n",
    "df['cyl'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min (Lowest numeric vaLue) of all column values \n",
    "df['cyl'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max (highest numeric vaLue) of all column values \n",
    "df['cyl'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Deviation of all column values \n",
    "df['cyl'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Columns \n",
    "\n",
    "**Note:** These are just the very basic sort operations. There are many other advanced methods (multi-column sort, index sort, etc) that include multiple arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe by column values \n",
    "df.sort_values('mpg', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Column Sort \n",
    "df.sort_values(['mpg', 'displ']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging DataFrames \n",
    "\n",
    "While many of these are similar, there are specifics and numerous arguments that can be used in conjunction that truly customize the type of DataFrame joining/merging/appending/concating you're trying to accomplish. \n",
    "\n",
    "**Note:** We've provided more sample DataFrames (a, b, c) to help illustrate the various methods. Join/Merge act similar to SQL joins. This Wikipedia entry might help but it can take some time to learn and get comfortable with using them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example df's \n",
    "a = pd.DataFrame([[1,2,3], [3,4,5]], columns=list('ABC')) \n",
    "b = pd.DataFrame([[5,2,3],[7,4,5]], columns=list('BDE')) \n",
    "c = pd.DataFrame([[11,12,13],[17,14,15]], columns=list('XYZ')) \n",
    "print(a) \n",
    "print(b) \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append DataFrames \n",
    "\n",
    "Merges 2+ DataFrames, Does not care if dissimilar or similar. Can also use a list of DataFrames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = a.append(b) \n",
    "ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate DataFrames \n",
    "\n",
    "Simlar to append, but handles large lists of dataframes well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = pd.concat([a,b,c]) \n",
    "abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join DataFrames \n",
    "\n",
    "SQL-ish join operations (Inner/Outer etc), can specify join on index, similar columns may require specification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = a.join(b,how='left',lsuffix=\"_a\",rsuffix=\"_b\") \n",
    "joined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge DataFrames \n",
    "\n",
    "Merges 2+ DataFrames with overlapping columns, Very similar to join. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = a.merge(b, left_on='B', right_on='D') \n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate across DataFrames\n",
    "\n",
    "Iterating is only good for small dataframes, larger dataframes generally require apply/map and functions for efficiency.\n",
    "\n",
    "You will inevitably use these methods at one point or another, but it's important to remember that dataframes aren't like most of the objects you've used before now. If you think you have to iterate across your dataframe to get something done there is probably a better way to do what you're trying to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iter Rows \n",
    "\n",
    "Access to values is done by index\n",
    "\n",
    "rows[0] = Index \n",
    "\n",
    "rows[1] = values as pandas series (similar to a diet) \n",
    "\n",
    "rows[1][0] = First column value of row, can specify column rows[1]['Column']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0 \n",
    "for row in df.iterrows(): \n",
    "    counter += 1\n",
    "    if counter > 15: \n",
    "        break \n",
    "    print(row[1].keys()[0]) \n",
    "    print(row[1]['name']) \n",
    "    print(row[0], row[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IterTuples \n",
    "\n",
    "Faster and more efficient, access to values is slightly different from iterrows (Index is not nested). \n",
    "\n",
    "rowtuples[0] = Index \n",
    "\n",
    "rowtuples[1] = First column value \n",
    "\n",
    "rowtuples[2] = Second column value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0 \n",
    "for rowtuples in df.itertuples(): \n",
    "    counter += 1 \n",
    "    if counter > 15: \n",
    "        break \n",
    "    print(rowtuples[0], rowtuples[1],rowtuples[2],rowtuples[3], rowtuples[4], rowtuples[5], rowtuples[6], rowtuples[7], rowtuples[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting on DataFrame \n",
    "\n",
    "Create Excel style pivot tables based on specified criteria "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Pivot \n",
    "df.pivot_table(index=['mpg','name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify for a more complex pivot table \n",
    "df.pivot_table(values=['weight'], index=['cyl','name'], aggfunc=np.mean).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Indexing \n",
    "\n",
    "Filter DataFrame on Multiple Columns and Values using Boolean index \n",
    "\n",
    "**Note:** The '&' in this example represents 'and' which might cause confusion. The explanation for this can also be a bit confusing, at least it caught the author off guard the first few times. The '&' will create a boolean array (of True/False which is used by the filtering operation to construct the output. When all 3 statements below return true for a row, pandas knows that we want that row in our output. The 'and' comparator functions differently than \n",
    "'&' and will throw a 'the truth value for the array is ambiguous' exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['cyl'] < 6) & \n",
    "       (df['mpg'] > 35)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same thing can be done with .query for a more SQL-esque way to do it \n",
    "# just beware that you can run into issues with string formatting when using this method \n",
    "df.query(\"cyl < 6 & mpg > 35\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crosstab Viewing \n",
    "\n",
    "Contingency table (also known as a cross tabulation or crosstab) is a type of table in a matrix format that displays the (multivariate) frequency distribution of the variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['cyl'],df['yr'],margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example using multiple options \n",
    "\n",
    "**Note:** This is an example using a combination of techniques seen above. We've also introduced a new method `.nlargest` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Number of Column1 Unique Values based on the Mean of NumColumn Unique Values using .nlargest \n",
    "df.cyl.value_counts().nlargest(math.ceil(df.mpg.value_counts().mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a New Column with simple logic \n",
    "\n",
    "Useful technique for simple operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using.astype(str) we can treat the float64 df['mpg'] column as a string and merge it with other strings \n",
    "df2 = df.copy()\n",
    "df2['mpg_str'] = df2['name'] + ' Has MPG ' + df2['mpg'].astype(str)\n",
    "df2.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions on DataFrames \n",
    "\n",
    "The fastest and most effecient method of running calculations against an entire dataframe. This will become your new method of 'iterating' over the data and doing analytics. \n",
    "\n",
    "axis = 0 means function will be applied to each column \n",
    "\n",
    "axis = 1 means function will be applied to each row \n",
    "\n",
    "**Note:** This is a step into more advanced techniques. Map/Apply/Applymap are the most efficient Pandas method of iterating and running functions across a DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map \n",
    "\n",
    "Map applys a function to each element in a series, very much like iterating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concon(x): \n",
    "    return 'Adding this String to all values: ' + str(x)\n",
    "\n",
    "df['name'].map(concon).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply \n",
    "\n",
    "Apply runs a function against the axis specified. \n",
    "\n",
    "We are creating hp_and_mpg based on results of adding.\n",
    "\n",
    "We are creating a New_Column based on the results of summing Column1 + Column2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['hp_and_mpg'] = df2[['hp', 'mpg']].apply(sum,axis=1) \n",
    "df2.loc[:, ['hp', 'mpg', 'hp_and_mpg', 'name']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ApplyMap \n",
    "\n",
    "Runs a function against each element in a dataframe (each 'cell') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.applymap(concon).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Function Examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_missing(x): \n",
    "    return sum(x.isnull()) \n",
    "\n",
    "# Check how many missing values in each column \n",
    "df.apply(num_missing, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many missing values in each row \n",
    "df.apply(num_missing, axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python 3 and Map \n",
    "\n",
    "**Note:** Similar to zip, map can return an object (instead of a value) depending on how it's configured. For both zip and map, you can use list() to get the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Example1(stuff):\n",
    "    return stuff + ' THINGS'\n",
    "\n",
    "# Try this without List, obverse the NewColumn values which are returned as objects \n",
    "df2 = df.copy()\n",
    "df2['NewColumn'] = map(Example1, df2['name'])\n",
    "df2.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try with a list, problem solved when using this syntax \n",
    "df2 = df.copy() \n",
    "df2['NewColumn'] = list(map(Example1, df2['name']))\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Multi-Column Functions \n",
    "\n",
    "**Note:** This is a technique to modify or create multiple columns based on a function that outputs multiple values in a tuple. We've written this to work directly with the sample DataFrame imported at the beginning of this resource guide. \n",
    "\n",
    "Example2 outputs a tuple of (x, y, z) which we unpack from map using * and then zip inline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Example2(one, two, three):\n",
    "    text = ' Text '\n",
    "    x = ''.join([str(one), text, str(two), text, str(three)])\n",
    "    y = sum([one, two, three]) \n",
    "    z = 'Poptarts' \n",
    "    return x, y, z \n",
    "\n",
    "df2 = df.copy() \n",
    "\n",
    "df2['StrColumn'], df2['SumColumn'], df2['PopColumn'] = zip(*map(\n",
    "    Example2, df2['mpg'], df2['cyl'], df2['hp'])) \n",
    "\n",
    "df2.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditionally Updating Values \n",
    "\n",
    "Use `.loc` to update values where a certain condition has been met. This is analogous to `SET ... WHERE ...` syntax in SQL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2['efficiency'] = \"\"\n",
    "# in SQL, \"UPDATE <tablename> SET efficiency = 'poor' WHERE mpg < 10\"\n",
    "df2.loc[(df2.mpg < 10), 'efficiency'] = \"poor\"\n",
    "df2.loc[(df2.mpg >= 10) & (df2.mpg < 30), 'efficiency'] = \"medium\"\n",
    "df2.loc[(df2.mpg >= 30), 'efficiency'] = \"high\"\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy and Aggregate \n",
    "\n",
    "Pandas makes it pretty simply to group your dataframe on a value or values, and then aggregate the other results. It's a little less flexible than SQL in some ways, but still pretty powerful. There's a lot you can do in Pandas with GroupBy objects, so definitely check the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting as_index to False will keep the grouped values as \n",
    "# regular columns values rather than indices \n",
    "grouped_df = df.groupby(by=['cyl'])\n",
    "\n",
    "# use.agg to aggregate the values and run specified functions \n",
    "# note that Me can't create new columns here \n",
    "aggregated = grouped_df.agg({\n",
    "    'mpg': np.mean,\n",
    "    'displ' : np.mean,\n",
    "    'hp' : np.mean,\n",
    "    'yr': np.max,\n",
    "    'accel': 'mean'\n",
    "}) \n",
    "aggregated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transcribed from FOIA Doc ID: 6689695\n",
    "\n",
    "https://archive.org/details/comp3321"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
